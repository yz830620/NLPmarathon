{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128,256)\n",
    "        self.layer3 = LinearBNAC(256,64)\n",
    "        self.layer4 = LinearBNAC(64, 32)\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備輸入資料、優化器、標籤資料、模型輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=256,output_classes=10)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_features = 256\n",
    "dummy_input = torch.randn(batch_size, input_features,)\n",
    "\n",
    "#target = torch.empty(4, dtype=torch.float).random_(10)\n",
    "target = torch.tensor([9., 5., 4., 4.], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.2604, 0.0803, 0.0698, 0.0768, 0.0577, 0.0467, 0.1006, 0.1674, 0.0841,\n         0.0561],\n        [0.0940, 0.1246, 0.0746, 0.1233, 0.0955, 0.0661, 0.0601, 0.0963, 0.1273,\n         0.1381],\n        [0.1248, 0.0903, 0.1302, 0.0776, 0.1519, 0.0396, 0.0795, 0.1231, 0.0926,\n         0.0904],\n        [0.0752, 0.0761, 0.1104, 0.1441, 0.0783, 0.1187, 0.0733, 0.1862, 0.0806,\n         0.0571]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算 CrossEntropy Loss\n",
    "* 請注意哪一個 Loss最適合：我們已經使用 softmax\n",
    "* 因為我們有使用dropout，並隨機產生dummy_input，所以各為學員得到的值會與解答不同，然而步驟原理需要相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(torch.log(output), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.5074, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成back propagation並更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weight : Parameter containing:\ntensor([[ 0.0483, -0.0293, -0.0423,  ...,  0.0426, -0.0160, -0.0508],\n        [ 0.0396, -0.0108, -0.0328,  ...,  0.0317,  0.0289,  0.0034],\n        [ 0.0489, -0.0236,  0.0154,  ...,  0.0439,  0.0460, -0.0065],\n        ...,\n        [ 0.0448,  0.0290,  0.0222,  ..., -0.0246,  0.0211,  0.0377],\n        [ 0.0096, -0.0469,  0.0582,  ..., -0.0443, -0.0579, -0.0447],\n        [ 0.0368,  0.0218, -0.0507,  ..., -0.0300,  0.0015,  0.0166]],\n       requires_grad=True)\n\n\ngrad : tensor([[ 1.5044e-01, -2.6275e-01,  3.9103e-02,  ..., -4.6727e-02,\n         -5.8239e-02, -1.3566e-01],\n        [-1.5740e-07, -1.3140e-07, -7.4710e-08,  ..., -1.9736e-08,\n          2.8783e-09,  6.4984e-08],\n        [-1.3266e-02,  4.4235e-03, -9.8266e-03,  ...,  1.3352e-04,\n         -7.0837e-03,  1.3571e-02],\n        ...,\n        [-3.9177e-02,  1.8807e-02,  3.6561e-02,  ..., -1.6867e-02,\n         -4.9197e-03, -2.3362e-02],\n        [ 1.4734e-03,  2.0869e-03, -1.0921e-03,  ...,  2.5712e-03,\n          6.3326e-03,  1.0970e-03],\n        [-2.2610e-01, -5.5099e-02, -7.1832e-02,  ..., -4.0863e-03,\n          6.1935e-02,  9.3005e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weight : Parameter containing:\ntensor([[ 0.0473, -0.0283, -0.0433,  ...,  0.0436, -0.0150, -0.0498],\n        [ 0.0386, -0.0098, -0.0318,  ...,  0.0307,  0.0279,  0.0024],\n        [ 0.0499, -0.0246,  0.0164,  ...,  0.0429,  0.0470, -0.0075],\n        ...,\n        [ 0.0458,  0.0280,  0.0212,  ..., -0.0236,  0.0221,  0.0387],\n        [ 0.0086, -0.0479,  0.0592,  ..., -0.0453, -0.0589, -0.0457],\n        [ 0.0378,  0.0228, -0.0497,  ..., -0.0290,  0.0005,  0.0156]],\n       requires_grad=True)\n\n\ngrad : tensor([[ 1.5044e-01, -2.6275e-01,  3.9103e-02,  ..., -4.6727e-02,\n         -5.8239e-02, -1.3566e-01],\n        [-1.5740e-07, -1.3140e-07, -7.4710e-08,  ..., -1.9736e-08,\n          2.8783e-09,  6.4984e-08],\n        [-1.3266e-02,  4.4235e-03, -9.8266e-03,  ...,  1.3352e-04,\n         -7.0837e-03,  1.3571e-02],\n        ...,\n        [-3.9177e-02,  1.8807e-02,  3.6561e-02,  ..., -1.6867e-02,\n         -4.9197e-03, -2.3362e-02],\n        [ 1.4734e-03,  2.0869e-03, -1.0921e-03,  ...,  2.5712e-03,\n          6.3326e-03,  1.0970e-03],\n        [-2.2610e-01, -5.5099e-02, -7.1832e-02,  ..., -4.0863e-03,\n          6.1935e-02,  9.3005e-02]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清空 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weight : Parameter containing:\ntensor([[ 0.0473, -0.0283, -0.0433,  ...,  0.0436, -0.0150, -0.0498],\n        [ 0.0386, -0.0098, -0.0318,  ...,  0.0307,  0.0279,  0.0024],\n        [ 0.0499, -0.0246,  0.0164,  ...,  0.0429,  0.0470, -0.0075],\n        ...,\n        [ 0.0458,  0.0280,  0.0212,  ..., -0.0236,  0.0221,  0.0387],\n        [ 0.0086, -0.0479,  0.0592,  ..., -0.0453, -0.0589, -0.0457],\n        [ 0.0378,  0.0228, -0.0497,  ..., -0.0290,  0.0005,  0.0156]],\n       requires_grad=True)\n\n\ngrad : tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}