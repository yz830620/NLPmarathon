{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.7.3 64-bit ('tf21g': conda)","metadata":{"interpreter":{"hash":"bce34ad92f12188bdbde6c4d0d0df6759e2da82e2ea2ad01676467783730534f"}}},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3-final"},"colab":{"name":"微分與倒傳遞作業.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"GxtArZ7u16sr"},"source":["### 作業目標: 使用Pytorch進行微分與倒傳遞\n","這份作業我們會實作微分與倒傳遞以及使用Pytorch的Autograd。"]},{"cell_type":"markdown","metadata":{"id":"q_BwC7sg16ss"},"source":["### 使用Pytorch實作微分與倒傳遞\n","\n","這裡我們很簡單的實作兩層的神經網路進行回歸問題，其中loss function為L2 loss\n","\n","$$\n","L2\\_loss = (y_{pred}-y)^2\n","$$\n","\n","兩層經網路如下所示\n","$$\n","y_{pred} = ReLU(XW_1)W_2\n","$$"]},{"cell_type":"code","metadata":{"id":"ocsA8ch-16st"},"source":["import torch\n","import time\n","device = torch.device('cuda')"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2v8hkG616sz","outputId":"0b737d18-59c2-4bb7-f541-e0ca6ab51a11","tags":[]},"source":["# N: batch size\n","# D_in: input dimension\n","# H: hidden dimension\n","# D_out: output dimension\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","# 隨機生成x, y\n","x = torch.randn(N, D_in,device=device)\n","y = torch.randn(N, D_out,device=device)\n","# 初始化weight W1, W2\n","W1 = torch.randn(D_in, H, device=device)\n","W2 = torch.randn(H, D_out, device=device)\n","\n","# 設置learning rate\n","lr = 1e-6\n","t1 = time.time()\n","# 訓練500個epoch\n","for t in range(501):\n","  # 向前傳遞: 計算y_pred\n","  h = x.mm(W1)\n","  y_pred= h.mm(W2)\n","\n","  # 計算loss\n","  loss = (y_pred - y).pow(2).sum()\n","  if t % 50 == 0:\n","    print(f'round {t}, loss: {loss.item()}')\n","\n","  # 倒傳遞: 計算W1與W2對loss的微分(梯度)\n","  grad_y_pred = 2.0 * (y_pred - y)\n","  grad_w2 = h.t().mm(grad_y_pred)\n","  grad_h = grad_y_pred.mm(W2.t())\n","  grad_w1 = x.t().mm(grad_h)\n","\n","  # 參數更新\n","  W1 = W1 - lr * grad_w1\n","  W2 = W2 - lr * grad_w2\n","print(f'Takes {round(time.time() - t1,2)}s to finish')"],"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["round 0, loss: 71942704.0\n","round 50, loss: 140.9893035888672\n","round 100, loss: 0.34629976749420166\n","round 150, loss: 0.0014960408443585038\n","round 200, loss: 4.160517346463166e-05\n","round 250, loss: 8.722498932911549e-06\n","round 300, loss: 3.6732722037413623e-06\n","round 350, loss: 2.1441210265038535e-06\n","round 400, loss: 1.4016502518643392e-06\n","round 450, loss: 9.892359003060847e-07\n","round 500, loss: 7.624586828569591e-07\n","Takes 0.33s to finish\n"]}]},{"cell_type":"markdown","metadata":{"id":"i9XiShaU16s3"},"source":["### 使用Pytorch的Autograd"]},{"cell_type":"code","metadata":{"id":"_VP1YW7516s4"},"source":["import torch\n","device = torch.device('cpu')"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"dlj3NwsP16s6","outputId":"0463fd34-3edf-4516-9d36-c1143463790d"},"source":["# N: batch size\n","# D_in: input dimension\n","# H: hidden dimension\n","# D_out: output dimension\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","# 隨機生成x, y\n","x = torch.randn(N, D_in,device=device)\n","y = torch.randn(N, D_out,device=device)\n","# 初始化weight W1, W2\n","W1 = torch.randn(D_in, H, device=device, requires_grad=True)\n","W2 = torch.randn(H, D_out, device=device, requires_grad=True)\n","\n","# 設置learning rate\n","lr = 1e-6\n","t1 = time.time()\n","# 訓練500個epoch\n","for t in range(501):\n","  # 向前傳遞: 計算y_pred\n","  y_pred = x.mm(W1).clamp(min=0).mm(W2)\n","  \n","  # 計算loss\n","  loss = (y_pred - y).pow(2).sum()\n","  if t % 50 == 0:\n","    print(f'round {t}, loss: {loss.item()}')\n","\n","  # 倒傳遞: 計算W1與W2對loss的微分(梯度)\n","  loss.backward()\n","\n","  # 參數更新: 這裡再更新參數時，我們不希望更新參數的計算也被紀錄微分相關的資訊，因此使用torch.no_grad()\n","  with torch.no_grad():\n","    # 更新參數W1 W2\n","    W1 -= lr * W1.grad\n","    W2 -= lr * W2.grad\n","\n","    # 將紀錄的gradient清空(因為已經更新參數)\n","    W1.grad.zero_()\n","    W2.grad.zero_()\n","print(f'Takes {round(time.time() - t1,2)}s to finish')"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["round 0, loss: 35622304.0\n","round 50, loss: 14778.615234375\n","round 100, loss: 373.82574462890625\n","round 150, loss: 13.941972732543945\n","round 200, loss: 0.6314912438392639\n","round 250, loss: 0.03267338126897812\n","round 300, loss: 0.002095188247039914\n","round 350, loss: 0.0002926356391981244\n","round 400, loss: 8.399760554311797e-05\n","round 450, loss: 3.687238859129138e-05\n","round 500, loss: 2.0867588318651542e-05\n","Takes 0.41s to finish\n"]}]},{"cell_type":"code","metadata":{"id":"znJFnEdr16s9"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}