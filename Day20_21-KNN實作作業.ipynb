{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MRC0e0KhQ0S"
   },
   "source": [
    "# K-Nearest Neighbors (K-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參考課程實作並在datasets_483_982_spam.csv的資料集中獲得90% 以上的 accuracy (testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWd1UlMnhT2s"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvGPUQaHhXfL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1VMqkGvhc3-"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "#\"讀取資料集\"\n",
    "dataset = pd.read_csv(r'datasets_483_982_spam.csv', encoding = 'latin-1')\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取出訓練內文與標註"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "label = le.fit_transform(dataset['v1'])\n",
    "dataset['v1'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.loc[:,'v2']\n",
    "Y = dataset.loc[:,'v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Data Examples : \n0    Go until jurong point, crazy.. Available only ...\n1                        Ok lar... Joking wif u oni...\n2    Free entry in 2 a wkly comp to win FA Cup fina...\n3    U dun say so early hor... U c already then say...\n4    Nah I don't think he goes to usf, he lives aro...\nName: v2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('Training Data Examples : \\n{}'.format(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Labeling Data Examples : \n0    0\n1    0\n2    1\n3    0\n4    0\nName: v1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Labeling Data Examples : \\n{}'.format(Y[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文字預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/evenpan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\"\"\"可以參考課程練習方式清理文字，或是使用自己的方式\"\"\"\n",
    "def get_pos_tag(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {'J': wordnet.ADJ,\n",
    "                'N': wordnet.NOUN,\n",
    "                'V': wordnet.VERB,\n",
    "                'R': wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def get_clean_text(Corpus):\n",
    "    Corpus_clean = [re.sub('[^a-zA-z]', ' ',corpus).lower() for corpus in Corpus]\n",
    "    Corpus_tokenize = [nltk.word_tokenize(corpus) for corpus in Corpus_clean]\n",
    "    Corpus_stopwords_lammatizer = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for content in Corpus_tokenize:\n",
    "        content_clean = []\n",
    "        for word in content:\n",
    "            if word not in stop_words:\n",
    "                word = lemmatizer.lemmatize(word, get_pos_tag(word))\n",
    "                content_clean.append(word)\n",
    "        Corpus_stopwords_lammatizer.append(content_clean)\n",
    "    Corpus_output = [' '.join(corpus) for corpus in Corpus_stopwords_lammatizer]\n",
    "    return Corpus_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_clean_text(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#max_features是要建造幾個column，會按造字出現的高低去篩選 \n",
    "cv=CountVectorizer(max_features = 1000)\n",
    "X=cv.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YvxIPVyMhmKp"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVzJWAXIhxoC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "設置max_feature值:1000\n",
      "設置K值:5\n",
      "Average Accuracy: 0.9326870559782335\n",
      "Accuracy STD: 0.011142883448628004\n",
      "設置max_feature值:1000\n",
      "設置K值:10\n",
      "Average Accuracy: 0.9073346097647\n",
      "Accuracy STD: 0.009748916670463811\n",
      "設置max_feature值:1000\n",
      "設置K值:25\n",
      "Average Accuracy: 0.8801869300146118\n",
      "Accuracy STD: 0.005265672355040464\n",
      "設置max_feature值:1000\n",
      "設置K值:50\n",
      "Average Accuracy: 0.8667269612535901\n",
      "Accuracy STD: 0.001032051062554304\n",
      "設置max_feature值:2000\n",
      "設置K值:5\n",
      "Average Accuracy: 0.9246112762634151\n",
      "Accuracy STD: 0.007470854281771815\n",
      "設置max_feature值:2000\n",
      "設置K值:10\n",
      "Average Accuracy: 0.8988093918476345\n",
      "Accuracy STD: 0.00692921751678977\n",
      "設置max_feature值:2000\n",
      "設置K值:25\n",
      "Average Accuracy: 0.8718869350531566\n",
      "Accuracy STD: 0.0033905170727354505\n",
      "設置max_feature值:2000\n",
      "設置K值:50\n",
      "Average Accuracy: 0.8660543155136796\n",
      "Accuracy STD: 0.0008912552114944287\n",
      "設置max_feature值:3000\n",
      "設置K值:5\n",
      "Average Accuracy: 0.9225923313347106\n",
      "Accuracy STD: 0.007469001136575767\n",
      "設置max_feature值:3000\n",
      "設置K值:10\n",
      "Average Accuracy: 0.8956698745402327\n",
      "Accuracy STD: 0.006668910790140812\n",
      "設置max_feature值:3000\n",
      "設置K值:25\n",
      "Average Accuracy: 0.8703159167632387\n",
      "Accuracy STD: 0.002219739733835909\n",
      "設置max_feature值:3000\n",
      "設置K值:50\n",
      "Average Accuracy: 0.8660543155136796\n",
      "Accuracy STD: 0.0008912552114944287\n",
      "設置max_feature值:5000\n",
      "設置K值:5\n",
      "Average Accuracy: 0.9221444046959238\n",
      "Accuracy STD: 0.008156491924217342\n",
      "設置max_feature值:5000\n",
      "設置K值:10\n",
      "Average Accuracy: 0.8940993601048017\n",
      "Accuracy STD: 0.007421479830771627\n",
      "設置max_feature值:5000\n",
      "設置K值:25\n",
      "Average Accuracy: 0.8712142893132462\n",
      "Accuracy STD: 0.0022573518314876946\n",
      "設置max_feature值:5000\n",
      "設置K值:50\n",
      "Average Accuracy: 0.8660543155136796\n",
      "Accuracy STD: 0.0008912552114944287\n"
     ]
    }
   ],
   "source": [
    "max_features = [1000, 2000, 3000, 5000]\n",
    "n_neighbors  = [5, 10, 25, 50] ## 可自行嘗試不同K值\n",
    "for feature in max_features:\n",
    "    X = dataset.loc[:,'v2']\n",
    "    X = get_clean_text(X)\n",
    "    cv=CountVectorizer(max_features = feature)\n",
    "    X=cv.fit_transform(X).toarray()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "    for k in n_neighbors:\n",
    "        classifier = KNeighborsClassifier(n_neighbors = k, metric = 'minkowski', p = 2)\n",
    "        # cv = 10 代表切成10等分\n",
    "        accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,n_jobs=-1)\n",
    "        print('設置max_feature值:{}'.format(feature))\n",
    "        print('設置K值:{}'.format(k))\n",
    "        print('Average Accuracy: {}'.format(accuracies.mean()))\n",
    "        print('Accuracy STD: {}'.format(accuracies.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bb6jCOCQiAmP"
   },
   "source": [
    "## Training the K-NN model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2356,
     "status": "ok",
     "timestamp": 1588492962262,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "e0pFVAmciHQs",
    "outputId": "8cb18c23-669b-452a-9bee-b2f96534f0f5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyxW5b395mR2"
   },
   "source": [
    "## Predicting a new result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2351,
     "status": "ok",
     "timestamp": 1588492962263,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "f8YOXsQy58rP",
    "outputId": "e248f6c5-4613-4a9e-faed-093c46defda1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trainset Accuracy: 0.9385236706304689\n"
     ]
    }
   ],
   "source": [
    "print('Trainset Accuracy: {}'.format(classifier.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testset Accuracy: 0.9219730941704036\n"
     ]
    }
   ],
   "source": [
    "print('Testset Accuracy: {}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKYVQH-l5NpE"
   },
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2345,
     "status": "ok",
     "timestamp": 1588492962263,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "p6VMTb2O4hwM",
    "outputId": "14b859cb-16df-4e5d-894b-3bda8e756d3d"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4Hwj34ziWQW"
   },
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3505,
     "status": "ok",
     "timestamp": 1588492963427,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "D6bpZwUiiXic",
    "outputId": "ec9468d5-c478-4ffa-ba1c-535eb56d7304"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[965   0]\n [ 87  63]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9219730941704036"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO/71HmJztjHpR9Q3DXpRZQ",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "k_nearest_neighbors.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}